{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0a89c0-807b-4eba-9979-fe96ed04cc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-13 12:18:24,032 - INFO - AxyaPreprocessor - === Pipeline Initialized ===\n",
      "2025-05-13 12:18:24,032 - INFO - AxyaPreprocessor - Starting API credits: 0/250\n",
      "2025-05-13 12:18:24,033 - INFO - AxyaPreprocessor - Loading raw data...\n",
      "2025-05-13 12:18:24,625 - INFO - AxyaPreprocessor - Processing 495 tickers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compliance Analysis:  40%|‚ñç| 197/495 [06:50<10:39,  2.15s/it, Compliant=134, Use"
     ]
    }
   ],
   "source": [
    "# 02_data_cleaning_preprocessing.py - Optimized Rate Limiting\n",
    "# Axya Quant Platform - Notebook 02: Preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import sys\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_PATH = Path('./data')\n",
    "RAW_PATH = DATA_PATH\n",
    "PROCESSED_PATH = DATA_PATH / 'processed'\n",
    "PROCESSED_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FMP_API_KEY = ''\n",
    "FMP_BASE_URL = ''\n",
    "MAX_DAILY_REQUESTS = 250\n",
    "INITIAL_USED = 0  # Start from fresh count\n",
    "REQUEST_INTERVAL = 1.2  # 1.2 seconds between requests (50/min)\n",
    "MAX_RETRIES = 3\n",
    "RETRY_BACKOFF = 5  # Seconds to wait after 429 error\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(DATA_PATH / 'preprocessing.log'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"AxyaPreprocessor\")\n",
    "\n",
    "# --- Data Loading ---\n",
    "def load_raw_data() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Load and validate input data with type conversion\"\"\"\n",
    "    logger.info(\"Loading raw data...\")\n",
    "    \n",
    "    try:\n",
    "        stock_data = pd.read_parquet(RAW_PATH / 'ohlcv.parquet').astype({\n",
    "            'open': 'float32', 'high': 'float32', \n",
    "            'low': 'float32', 'close': 'float32',\n",
    "            'volume': 'int32'\n",
    "        })\n",
    "        stock_data['date'] = pd.to_datetime(stock_data['date'])\n",
    "        \n",
    "        macro_data = pd.read_parquet(RAW_PATH / 'macro_data.parquet')\n",
    "        macro_data['date'] = pd.to_datetime(macro_data['date'])\n",
    "\n",
    "        return stock_data, macro_data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Data loading failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# --- Enhanced API Handler ---\n",
    "class FMPRateController:\n",
    "    \"\"\"Advanced rate limiting with exponential backoff\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_used: int = 0):\n",
    "        self.used_credits = initial_used\n",
    "        self.last_request = 0\n",
    "        self.retries = 0\n",
    "        \n",
    "    def make_request(self, ticker: str) -> dict:\n",
    "        \"\"\"Safe API request with retry logic\"\"\"\n",
    "        if self.used_credits >= MAX_DAILY_REQUESTS:\n",
    "            return None\n",
    "\n",
    "        cache_path = DATA_PATH / 'fmp_cache' / f\"{ticker}_balance.json\"\n",
    "        if cache_path.exists():\n",
    "            try:\n",
    "                with open(cache_path) as f:\n",
    "                    data = json.load(f)\n",
    "                if self.validate_data(data):\n",
    "                    return data\n",
    "            except (json.JSONDecodeError, KeyError):\n",
    "                pass\n",
    "\n",
    "        # Rate limiting\n",
    "        elapsed = time.time() - self.last_request\n",
    "        if elapsed < REQUEST_INTERVAL:\n",
    "            time.sleep(REQUEST_INTERVAL - elapsed)\n",
    "\n",
    "        url = f\"{FMP_BASE_URL}/balance-sheet-statement/{ticker}\"\n",
    "        params = {'period': 'annual', 'apikey': FMP_API_KEY}\n",
    "        \n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            try:\n",
    "                response = requests.get(url, params=params, timeout=15)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                data = response.json()\n",
    "                if isinstance(data, list) and len(data) > 0:\n",
    "                    valid_data = data[0] if self.validate_data(data[0]) else None\n",
    "                    if valid_data:\n",
    "                        self.used_credits += 1\n",
    "                        self.last_request = time.time()\n",
    "                        cache_path.parent.mkdir(exist_ok=True)\n",
    "                        with open(cache_path, 'w') as f:\n",
    "                            json.dump(valid_data, f)\n",
    "                        return valid_data\n",
    "                return None\n",
    "                \n",
    "            except requests.HTTPError as e:\n",
    "                if response.status_code == 429:\n",
    "                    wait_time = RETRY_BACKOFF * (2 ** attempt)\n",
    "                    logger.warning(f\"Rate limited: Retrying in {wait_time}s (Attempt {attempt+1})\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                logger.error(f\"HTTP error for {ticker}: {str(e)}\")\n",
    "                return None\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Request failed for {ticker}: {str(e)}\")\n",
    "                return None\n",
    "                \n",
    "        logger.error(f\"Max retries reached for {ticker}\")\n",
    "        return None\n",
    "\n",
    "    def validate_data(self, data: dict) -> bool:\n",
    "        \"\"\"Validate financial data structure\"\"\"\n",
    "        required = {\n",
    "            'symbol': str, 'date': str,\n",
    "            'totalAssets': (int, float), 'totalDebt': (int, float),\n",
    "            'cashAndShortTermInvestments': (int, float),\n",
    "            'netReceivables': (int, float)\n",
    "        }\n",
    "        return all(\n",
    "            isinstance(data.get(k), v) and (k != 'totalAssets' or data[k] > 0)\n",
    "            for k, v in required.items()\n",
    "        )\n",
    "\n",
    "# --- Compliance Analysis ---\n",
    "def analyze_compliance(tickers: List[str]) -> Tuple[Dict[str, bool], int]:\n",
    "    \"\"\"Run compliance checks with enhanced rate control\"\"\"\n",
    "    handler = FMPRateController(initial_used=INITIAL_USED)\n",
    "    results = {}\n",
    "    \n",
    "    with tqdm(total=len(tickers), desc=\"Compliance Analysis\") as pbar:\n",
    "        for ticker in tickers:\n",
    "            if handler.used_credits >= MAX_DAILY_REQUESTS:\n",
    "                logger.warning(\"Daily API limit reached - using cached data only\")\n",
    "                break\n",
    "\n",
    "            data = handler.make_request(ticker)\n",
    "            if data:\n",
    "                try:\n",
    "                    assets = float(data['totalAssets'])\n",
    "                    debt_ratio = float(data['totalDebt']) / assets\n",
    "                    receivables_ratio = float(data['netReceivables']) / assets\n",
    "                    compliant = debt_ratio < 0.33 and receivables_ratio < 0.49\n",
    "                except (ZeroDivisionError, KeyError):\n",
    "                    compliant = False\n",
    "            else:\n",
    "                compliant = False\n",
    "                \n",
    "            results[ticker] = compliant\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\n",
    "                'Compliant': sum(results.values()),\n",
    "                'Used': handler.used_credits,\n",
    "                'Remaining': MAX_DAILY_REQUESTS - handler.used_credits\n",
    "            })\n",
    "    \n",
    "    return results, handler.used_credits\n",
    "\n",
    "# --- Main Pipeline ---\n",
    "def main():\n",
    "    try:\n",
    "        logger.info(\"=== Pipeline Initialized ===\")\n",
    "        logger.info(f\"Starting API credits: {INITIAL_USED}/{MAX_DAILY_REQUESTS}\")\n",
    "\n",
    "        # Load and merge data\n",
    "        stock_df, macro_df = load_raw_data()\n",
    "        merged = pd.merge_asof(\n",
    "            stock_df.sort_values('date'),\n",
    "            macro_df.sort_values('date'),\n",
    "            on='date',\n",
    "            direction='nearest',\n",
    "            tolerance=pd.Timedelta('3D')\n",
    "        ).dropna(subset=['close', 'volume'])\n",
    "        \n",
    "        # Feature engineering\n",
    "        merged['returns_1d'] = merged.groupby('ticker')['close'].pct_change()\n",
    "        merged['volatility_5d'] = (\n",
    "            merged.groupby('ticker')['returns_1d']\n",
    "            .rolling(5, min_periods=3)\n",
    "            .std()\n",
    "            .reset_index(level=0, drop=True)\n",
    "        )\n",
    "\n",
    "        # Compliance analysis\n",
    "        tickers = merged['ticker'].unique().tolist()\n",
    "        logger.info(f\"Processing {len(tickers)} tickers\")\n",
    "        compliance_map, used_credits = analyze_compliance(tickers)\n",
    "        \n",
    "        # Apply results\n",
    "        merged['is_halal'] = (\n",
    "            merged['ticker']\n",
    "            .map(compliance_map)\n",
    "            .fillna(False)\n",
    "            .astype(bool)\n",
    "        )\n",
    "        \n",
    "        # Save outputs\n",
    "        merged.to_parquet(PROCESSED_PATH / 'full_dataset.parquet')\n",
    "        if merged['is_halal'].any():\n",
    "            merged[merged['is_halal']].to_parquet(PROCESSED_PATH / 'halal_instruments.parquet')\n",
    "        \n",
    "        logger.info(f\"Final API usage: {used_credits}/{MAX_DAILY_REQUESTS}\")\n",
    "        logger.info(f\"Compliance rate: {merged['is_halal'].mean():.2%}\")\n",
    "        logger.info(\"=== Pipeline Completed ===\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Pipeline failed: {str(e)}\", exc_info=True)\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd155e9-7dc2-48f5-a2c1-bf72dcef21bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
